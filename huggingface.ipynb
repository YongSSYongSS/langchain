{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense embbeding\n",
    " - ëª¨ë“  ì°¨ì›ì— ê°’ì´ ì¡´ì¬(dense = ì´˜ì´˜í•¨)\n",
    " - ëª¨ë“  ì°¨ì›ì´ í•­ìƒ ê°’ì„ ê°€ì§  [0.13, -0.27, 0.88, ..., 0.01] \n",
    " - ì˜ë¯¸ ê¸°ë°˜ í‘œí˜„ (ë‹¨ì–´, ë¬¸ì¥, ë¬¸ì„ ì˜ ì˜ë¯¸ë¥¼ ì••ì¶•)\n",
    " \n",
    "Sparse embedding\n",
    " - ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0(sparse = ë“œë¬¸ë“œë¬¸í•¨)\n",
    " - ë‹¨ì–´ ê¸°ë°˜(lexical)\n",
    " \n",
    "\n",
    "dense embeddingì€ \"í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ì••ì¶•í•´ì„œ í‘œí˜„\"   \n",
    "sparse embeddingì€ \"í…ìŠ¤íŠ¸ ë‚´ ì–´ë–¤ ë‹¨ì–´ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ ì¤‘ì‹¬ìœ¼ë¡œ í‘œí˜„\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "\n",
    "    \n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ìƒ˜í”Œë°ì´í„°í„°\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "texts = [\n",
    "    \"ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ê¹€ìš©í˜„ì…ë‹ˆë‹¤. lanchaingì—ì„œ ì„ë² ë”©ì„ ê³µë¶€ì¤‘ì— ìˆìŠµë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \t\tBAAI/bge-m3\n",
      "Dimension: \t1024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85eaa4a46d042b6985979f7884f414e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "bge_flagmodel = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\", use_fp16=True\n",
    ")  \n",
    "bge_encoded = bge_flagmodel.encode(texts, return_dense=True) #ì¼ë°˜ì ì¸ ì„ë² ë”©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense_vecs': array([[-0.0510617 , -0.00911605, -0.03279521, ..., -0.03671136,\n",
       "          0.03529508,  0.00833996]], dtype=float32),\n",
       " 'lexical_weights': None,\n",
       " 'colbert_vecs': None}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bge_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acad925328904fa6935e40e0668d449b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "bge_flagmodel = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\", use_fp16=True\n",
    ")  \n",
    "bge_encoded_sparse = bge_flagmodel.encode(texts, return_dense=False, return_sparse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense_vecs': array([[-0.0510617 , -0.00911605, -0.03279521, ..., -0.03671136,\n",
       "          0.03529508,  0.00833996]], dtype=float32),\n",
       " 'lexical_weights': [defaultdict(int,\n",
       "              {'107687': 0.225349,\n",
       "               '5': 0.05269464,\n",
       "               '6600': 0.12692782,\n",
       "               '50932': 0.1826216,\n",
       "               '697': 0.12572032,\n",
       "               '8237': 0.21062742,\n",
       "               '5358': 0.25677747,\n",
       "               '14020': 0.27770653,\n",
       "               '5826': 0.14465418,\n",
       "               '2515': 0.10470968,\n",
       "               '1436': 0.17326719,\n",
       "               '214': 0.16522749,\n",
       "               '1180': 0.08300017,\n",
       "               '20945': 0.11303122,\n",
       "               '42431': 0.2170032,\n",
       "               '50260': 0.162927,\n",
       "               '413': 0.015961185,\n",
       "               '83773': 0.17251246,\n",
       "               '7094': 0.11248326,\n",
       "               '480': 0.04173658,\n",
       "               '3292': 0.073996305})],\n",
       " 'colbert_vecs': None}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bge_encoded_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14020 â†’ 'í˜„'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BGE-M3 ëª¨ë¸ê³¼ ë™ì¼í•œ tokenizer ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "# ì˜ˆì‹œ: sparse vector í‚¤ ì¤‘ í•˜ë‚˜\n",
    "token_id = 14020  # ì˜ˆ: sparse vectorì—ì„œ ë‚˜ì˜¨ í‚¤\n",
    "\n",
    "# ë‹¨ì¼ í† í° ë””ì½”ë”©\n",
    "token_str = tokenizer.decode([token_id])\n",
    "print(f\"{token_id} â†’ '{token_str}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¨ì–´ì˜ ê°€ì¤‘ì¹˜ì˜ ê¸°ì¤€ (bge-m3)\n",
    " 1. Term Frequency (TF)   \n",
    "    í•´ë‹¹ ë¬¸ì¥ì—ì„œ ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ìì£¼ ë“±ì¥í–ˆëŠ”ê°€\n",
    " 2. Inverse Document Frequency (IDF)   \n",
    "    ê·¸ ë‹¨ì–´ê°€ ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ ì–¼ë§ˆë‚˜ í¬ê·€í•œê°€\n",
    "\n",
    "- í”í•œ ë‹¨ì–´(ì€,ëŠ”,ì´,ê°€)ëŠ” ë‚®ì€ ê°€ì¤‘ì¹˜, ë¬¸ì„œì—ì„œë§Œ ë“±ì¥í•˜ëŠ” íŠ¹ì´í•œ ë‹¨ì–´ëŠ” ë†’ì€ ê°€ì¤‘ì¹˜\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì°¨ì´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¦ Dense ìœ ì‚¬ë„ (ì˜ë¯¸): 0.8880\n",
      "ğŸŸ§ Sparse ìœ ì‚¬ë„ (ë‹¨ì–´ ì¼ì¹˜): 0.2839\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "texts = [\"ì˜¤ëŠ˜ ë‚ ì”¨ ì°¸ ì¢‹ë„¤ìš”\", \"ì˜¤ëŠ˜ ê¸°ë¶„ ì •ë§ ì¢‹ì•„ìš”\"]\n",
    "\n",
    "# dense-only ì¸ì½”ë”©\n",
    "dense_encoded = bge_flagmodel.encode(texts, return_dense=True, return_sparse=False)\n",
    "\n",
    "# sparse-only ì¸ì½”ë”©\n",
    "sparse_encoded = bge_flagmodel.encode(texts, return_dense=False, return_sparse=True)\n",
    "\n",
    "# 1. DENSE ìœ ì‚¬ë„ ê³„ì‚° (cosine similarity)\n",
    "dense_vecs = dense_encoded[\"dense_vecs\"]\n",
    "dense_sim = cosine_similarity([dense_vecs[0]], [dense_vecs[1]])[0][0]\n",
    "\n",
    "# 2. SPARSE ìœ ì‚¬ë„ ê³„ì‚° (lexical_weights ê¸°ë°˜ ë‹¨ìˆœ dot product)\n",
    "# sparse ë²¡í„°ëŠ” dict í˜•íƒœì´ë¯€ë¡œ numpy vectorë¡œ ë³€í™˜\n",
    "def sparse_dict_to_vector(sparse_dict, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for i, token in enumerate(vocab):\n",
    "        vec[i] = sparse_dict.get(token, 0)\n",
    "    return vec\n",
    "\n",
    "vocab = list(set(sparse_encoded[\"lexical_weights\"][0].keys()) | \n",
    "             set(sparse_encoded[\"lexical_weights\"][1].keys()))\n",
    "\n",
    "v1 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][0], vocab)\n",
    "v2 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][1], vocab)\n",
    "\n",
    "sparse_sim = cosine_similarity([v1], [v2])[0][0]\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ğŸŸ¦ Dense ìœ ì‚¬ë„ (ì˜ë¯¸): {dense_sim:.4f}\")\n",
    "print(f\"ğŸŸ§ Sparse ìœ ì‚¬ë„ (ë‹¨ì–´ ì¼ì¹˜): {sparse_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03337294,  0.02527683, -0.00537117, ..., -0.03823467,\n",
       "        -0.01806252,  0.01654678],\n",
       "       [ 0.00020291,  0.01154577, -0.00948457, ..., -0.03402546,\n",
       "        -0.01071138,  0.04857172]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¦ Dense ìœ ì‚¬ë„ (ì˜ë¯¸): 0.7739\n",
      "ğŸŸ§ Sparse ìœ ì‚¬ë„ (ë‹¨ì–´ ì¼ì¹˜): 0.6959\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts = [\"ì € ì˜í™”ë°°ìš°ëŠ” ì •ë§ ëª» ìƒê²¼ë”ë¼\", \"ì € ì˜í™”ë°°ìš°ëŠ” ëª» í•˜ëŠ”ê²Œ ì—†ë”ë¼\"]\n",
    "\n",
    "# dense-only ì¸ì½”ë”©\n",
    "dense_encoded = bge_flagmodel.encode(texts, return_dense=True, return_sparse=False)\n",
    "\n",
    "# sparse-only ì¸ì½”ë”©\n",
    "sparse_encoded = bge_flagmodel.encode(texts, return_dense=False, return_sparse=True)\n",
    "\n",
    "# 1. DENSE ìœ ì‚¬ë„ ê³„ì‚° (cosine similarity)\n",
    "dense_vecs = dense_encoded[\"dense_vecs\"]\n",
    "dense_sim = cosine_similarity([dense_vecs[0]], [dense_vecs[1]])[0][0]\n",
    "\n",
    "# 2. SPARSE ìœ ì‚¬ë„ ê³„ì‚° (lexical_weights ê¸°ë°˜ ë‹¨ìˆœ dot product)\n",
    "# sparse ë²¡í„°ëŠ” dict í˜•íƒœì´ë¯€ë¡œ numpy vectorë¡œ ë³€í™˜\n",
    "def sparse_dict_to_vector(sparse_dict, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for i, token in enumerate(vocab):\n",
    "        vec[i] = sparse_dict.get(token, 0)\n",
    "    return vec\n",
    "\n",
    "vocab = list(set(sparse_encoded[\"lexical_weights\"][0].keys()) | \n",
    "             set(sparse_encoded[\"lexical_weights\"][1].keys()))\n",
    "\n",
    "v1 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][0], vocab)\n",
    "v2 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][1], vocab)\n",
    "\n",
    "sparse_sim = cosine_similarity([v1], [v2])[0][0]\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ğŸŸ¦ Dense ìœ ì‚¬ë„ (ì˜ë¯¸): {dense_sim:.4f}\")\n",
    "print(f\"ğŸŸ§ Sparse ìœ ì‚¬ë„ (ë‹¨ì–´ ì¼ì¹˜): {sparse_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¦ Dense ìœ ì‚¬ë„ (ì˜ë¯¸): 0.7900\n",
      "ğŸŸ§ Sparse ìœ ì‚¬ë„ (ë‹¨ì–´ ì¼ì¹˜): 0.6786\n"
     ]
    }
   ],
   "source": [
    "texts = [\"ì•„ë¹  ê°€ë°©ì— ë“¤ì–´ê°”ìŠµë‹ˆë‹¤.\", \"ì•„ë¹ ê°€ ë°©ì— ë“¤ì–´ê°”ìŠµë‹ˆë‹¤.\"]\n",
    "\n",
    "# dense-only ì¸ì½”ë”©\n",
    "dense_encoded = bge_flagmodel.encode(texts, return_dense=True, return_sparse=False)\n",
    "\n",
    "# sparse-only ì¸ì½”ë”©\n",
    "sparse_encoded = bge_flagmodel.encode(texts, return_dense=False, return_sparse=True)\n",
    "\n",
    "# 1. DENSE ìœ ì‚¬ë„ ê³„ì‚° (cosine similarity)\n",
    "dense_vecs = dense_encoded[\"dense_vecs\"]\n",
    "dense_sim = cosine_similarity([dense_vecs[0]], [dense_vecs[1]])[0][0]\n",
    "\n",
    "# 2. SPARSE ìœ ì‚¬ë„ ê³„ì‚° (lexical_weights ê¸°ë°˜ ë‹¨ìˆœ dot product)\n",
    "# sparse ë²¡í„°ëŠ” dict í˜•íƒœì´ë¯€ë¡œ numpy vectorë¡œ ë³€í™˜\n",
    "def sparse_dict_to_vector(sparse_dict, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for i, token in enumerate(vocab):\n",
    "        vec[i] = sparse_dict.get(token, 0)\n",
    "    return vec\n",
    "\n",
    "vocab = list(set(sparse_encoded[\"lexical_weights\"][0].keys()) | \n",
    "             set(sparse_encoded[\"lexical_weights\"][1].keys()))\n",
    "\n",
    "v1 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][0], vocab)\n",
    "v2 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][1], vocab)\n",
    "\n",
    "sparse_sim = cosine_similarity([v1], [v2])[0][0]\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ğŸŸ¦ Dense ìœ ì‚¬ë„ (ì˜ë¯¸): {dense_sim:.4f}\")\n",
    "print(f\"ğŸŸ§ Sparse ìœ ì‚¬ë„ (ë‹¨ì–´ ì¼ì¹˜): {sparse_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¦ Dense ìœ ì‚¬ë„ (ì˜ë¯¸): 0.7582\n",
      "ğŸŸ§ Sparse ìœ ì‚¬ë„ (ë‹¨ì–´ ì¼ì¹˜): 0.4282\n"
     ]
    }
   ],
   "source": [
    "texts = [\"ë‚´ê°€ ì œì¼ ì¡´ê²½í•˜ëŠ” ì¸ë¬¼ì€ ì´ìˆœì‹  ì •êµ°ì…ë‹ˆë‹¤.\", \"ì´ìˆœì‹  ì¥êµ°ì€ ì •ë§ ìœ„ëŒ€í•©ë‹ˆë‹¤.\"]\n",
    "\n",
    "# dense-only ì¸ì½”ë”©\n",
    "dense_encoded = bge_flagmodel.encode(texts, return_dense=True, return_sparse=False)\n",
    "\n",
    "# sparse-only ì¸ì½”ë”©\n",
    "sparse_encoded = bge_flagmodel.encode(texts, return_dense=False, return_sparse=True)\n",
    "\n",
    "# 1. DENSE ìœ ì‚¬ë„ ê³„ì‚° (cosine similarity)\n",
    "dense_vecs = dense_encoded[\"dense_vecs\"]\n",
    "dense_sim = cosine_similarity([dense_vecs[0]], [dense_vecs[1]])[0][0]\n",
    "\n",
    "# 2. SPARSE ìœ ì‚¬ë„ ê³„ì‚° (lexical_weights ê¸°ë°˜ ë‹¨ìˆœ dot product)\n",
    "# sparse ë²¡í„°ëŠ” dict í˜•íƒœì´ë¯€ë¡œ numpy vectorë¡œ ë³€í™˜\n",
    "def sparse_dict_to_vector(sparse_dict, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for i, token in enumerate(vocab):\n",
    "        vec[i] = sparse_dict.get(token, 0)\n",
    "    return vec\n",
    "\n",
    "vocab = list(set(sparse_encoded[\"lexical_weights\"][0].keys()) | \n",
    "             set(sparse_encoded[\"lexical_weights\"][1].keys()))\n",
    "\n",
    "v1 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][0], vocab)\n",
    "v2 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][1], vocab)\n",
    "\n",
    "sparse_sim = cosine_similarity([v1], [v2])[0][0]\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ğŸŸ¦ Dense ìœ ì‚¬ë„ (ì˜ë¯¸): {dense_sim:.4f}\")\n",
    "print(f\"ğŸŸ§ Sparse ìœ ì‚¬ë„ (ë‹¨ì–´ ì¼ì¹˜): {sparse_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse ì„ë² ë”©ì€ ë‹¨ì–´ê°€ ì •í™•íˆ í¬í•¨ë˜ì–´ ìˆëŠ”ëƒë¥¼ ë³´ê¸°ë•Œë¬¸ì—, ì‘ì€ ì˜¤íƒ€ë‚˜ í‘œí˜„ì°¨ì´ë„ ìœ ì‚¬ë„ë¥¼ í¬ê²Œ ë–¨ì–´ëœ¨ë¦´ ìˆ˜ ìˆìŒ.   \n",
    "ì´ ì ì€ dens embeddingê³¼ í¬ê²Œ ë‹¤ë¥¸ íŠ¹ì§•\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
