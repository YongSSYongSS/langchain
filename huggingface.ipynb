{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense embbeding\n",
    " - 모든 차원에 값이 존재(dense = 촘촘함)\n",
    " - 모든 차원이 항상 값을 가짐  [0.13, -0.27, 0.88, ..., 0.01] \n",
    " - 의미 기반 표현 (단어, 문장, 문선의 의미를 압축)\n",
    " \n",
    "Sparse embedding\n",
    " - 대부분의 값이 0(sparse = 드문드문함)\n",
    " - 단어 기반(lexical)\n",
    " \n",
    "\n",
    "dense embedding은 \"텍스트의 의미를 압축해서 표현\"   \n",
    "sparse embedding은 \"텍스트 내 어떤 단어가 포함되었는지 중심으로 표현\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "\n",
    "    \n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#샘플데이터터\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "texts = [\n",
    "    \"안녕하세요. 제 이름은 김용현입니다. lanchaing에서 임베딩을 공부중에 있습니다.\"\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \t\tBAAI/bge-m3\n",
      "Dimension: \t1024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85eaa4a46d042b6985979f7884f414e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "bge_flagmodel = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\", use_fp16=True\n",
    ")  \n",
    "bge_encoded = bge_flagmodel.encode(texts, return_dense=True) #일반적인 임베딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense_vecs': array([[-0.0510617 , -0.00911605, -0.03279521, ..., -0.03671136,\n",
       "          0.03529508,  0.00833996]], dtype=float32),\n",
       " 'lexical_weights': None,\n",
       " 'colbert_vecs': None}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bge_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acad925328904fa6935e40e0668d449b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "bge_flagmodel = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\", use_fp16=True\n",
    ")  \n",
    "bge_encoded_sparse = bge_flagmodel.encode(texts, return_dense=False, return_sparse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense_vecs': array([[-0.0510617 , -0.00911605, -0.03279521, ..., -0.03671136,\n",
       "          0.03529508,  0.00833996]], dtype=float32),\n",
       " 'lexical_weights': [defaultdict(int,\n",
       "              {'107687': 0.225349,\n",
       "               '5': 0.05269464,\n",
       "               '6600': 0.12692782,\n",
       "               '50932': 0.1826216,\n",
       "               '697': 0.12572032,\n",
       "               '8237': 0.21062742,\n",
       "               '5358': 0.25677747,\n",
       "               '14020': 0.27770653,\n",
       "               '5826': 0.14465418,\n",
       "               '2515': 0.10470968,\n",
       "               '1436': 0.17326719,\n",
       "               '214': 0.16522749,\n",
       "               '1180': 0.08300017,\n",
       "               '20945': 0.11303122,\n",
       "               '42431': 0.2170032,\n",
       "               '50260': 0.162927,\n",
       "               '413': 0.015961185,\n",
       "               '83773': 0.17251246,\n",
       "               '7094': 0.11248326,\n",
       "               '480': 0.04173658,\n",
       "               '3292': 0.073996305})],\n",
       " 'colbert_vecs': None}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bge_encoded_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14020 → '현'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BGE-M3 모델과 동일한 tokenizer 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "# 예시: sparse vector 키 중 하나\n",
    "token_id = 14020  # 예: sparse vector에서 나온 키\n",
    "\n",
    "# 단일 토큰 디코딩\n",
    "token_str = tokenizer.decode([token_id])\n",
    "print(f\"{token_id} → '{token_str}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 가중치의 기준 (bge-m3)\n",
    " 1. Term Frequency (TF)   \n",
    "    해당 문장에서 단어가 얼마나 자주 등장했는가\n",
    " 2. Inverse Document Frequency (IDF)   \n",
    "    그 단어가 전체 코퍼스에서 얼마나 희귀한가\n",
    "\n",
    "- 흔한 단어(은,는,이,가)는 낮은 가중치, 문서에서만 등장하는 특이한 단어는 높은 가중치\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟦 Dense 유사도 (의미): 0.8880\n",
      "🟧 Sparse 유사도 (단어 일치): 0.2839\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "texts = [\"오늘 날씨 참 좋네요\", \"오늘 기분 정말 좋아요\"]\n",
    "\n",
    "# dense-only 인코딩\n",
    "dense_encoded = bge_flagmodel.encode(texts, return_dense=True, return_sparse=False)\n",
    "\n",
    "# sparse-only 인코딩\n",
    "sparse_encoded = bge_flagmodel.encode(texts, return_dense=False, return_sparse=True)\n",
    "\n",
    "# 1. DENSE 유사도 계산 (cosine similarity)\n",
    "dense_vecs = dense_encoded[\"dense_vecs\"]\n",
    "dense_sim = cosine_similarity([dense_vecs[0]], [dense_vecs[1]])[0][0]\n",
    "\n",
    "# 2. SPARSE 유사도 계산 (lexical_weights 기반 단순 dot product)\n",
    "# sparse 벡터는 dict 형태이므로 numpy vector로 변환\n",
    "def sparse_dict_to_vector(sparse_dict, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for i, token in enumerate(vocab):\n",
    "        vec[i] = sparse_dict.get(token, 0)\n",
    "    return vec\n",
    "\n",
    "vocab = list(set(sparse_encoded[\"lexical_weights\"][0].keys()) | \n",
    "             set(sparse_encoded[\"lexical_weights\"][1].keys()))\n",
    "\n",
    "v1 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][0], vocab)\n",
    "v2 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][1], vocab)\n",
    "\n",
    "sparse_sim = cosine_similarity([v1], [v2])[0][0]\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"🟦 Dense 유사도 (의미): {dense_sim:.4f}\")\n",
    "print(f\"🟧 Sparse 유사도 (단어 일치): {sparse_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03337294,  0.02527683, -0.00537117, ..., -0.03823467,\n",
       "        -0.01806252,  0.01654678],\n",
       "       [ 0.00020291,  0.01154577, -0.00948457, ..., -0.03402546,\n",
       "        -0.01071138,  0.04857172]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟦 Dense 유사도 (의미): 0.7739\n",
      "🟧 Sparse 유사도 (단어 일치): 0.6959\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts = [\"저 영화배우는 정말 못 생겼더라\", \"저 영화배우는 못 하는게 없더라\"]\n",
    "\n",
    "# dense-only 인코딩\n",
    "dense_encoded = bge_flagmodel.encode(texts, return_dense=True, return_sparse=False)\n",
    "\n",
    "# sparse-only 인코딩\n",
    "sparse_encoded = bge_flagmodel.encode(texts, return_dense=False, return_sparse=True)\n",
    "\n",
    "# 1. DENSE 유사도 계산 (cosine similarity)\n",
    "dense_vecs = dense_encoded[\"dense_vecs\"]\n",
    "dense_sim = cosine_similarity([dense_vecs[0]], [dense_vecs[1]])[0][0]\n",
    "\n",
    "# 2. SPARSE 유사도 계산 (lexical_weights 기반 단순 dot product)\n",
    "# sparse 벡터는 dict 형태이므로 numpy vector로 변환\n",
    "def sparse_dict_to_vector(sparse_dict, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for i, token in enumerate(vocab):\n",
    "        vec[i] = sparse_dict.get(token, 0)\n",
    "    return vec\n",
    "\n",
    "vocab = list(set(sparse_encoded[\"lexical_weights\"][0].keys()) | \n",
    "             set(sparse_encoded[\"lexical_weights\"][1].keys()))\n",
    "\n",
    "v1 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][0], vocab)\n",
    "v2 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][1], vocab)\n",
    "\n",
    "sparse_sim = cosine_similarity([v1], [v2])[0][0]\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"🟦 Dense 유사도 (의미): {dense_sim:.4f}\")\n",
    "print(f\"🟧 Sparse 유사도 (단어 일치): {sparse_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟦 Dense 유사도 (의미): 0.7900\n",
      "🟧 Sparse 유사도 (단어 일치): 0.6786\n"
     ]
    }
   ],
   "source": [
    "texts = [\"아빠 가방에 들어갔습니다.\", \"아빠가 방에 들어갔습니다.\"]\n",
    "\n",
    "# dense-only 인코딩\n",
    "dense_encoded = bge_flagmodel.encode(texts, return_dense=True, return_sparse=False)\n",
    "\n",
    "# sparse-only 인코딩\n",
    "sparse_encoded = bge_flagmodel.encode(texts, return_dense=False, return_sparse=True)\n",
    "\n",
    "# 1. DENSE 유사도 계산 (cosine similarity)\n",
    "dense_vecs = dense_encoded[\"dense_vecs\"]\n",
    "dense_sim = cosine_similarity([dense_vecs[0]], [dense_vecs[1]])[0][0]\n",
    "\n",
    "# 2. SPARSE 유사도 계산 (lexical_weights 기반 단순 dot product)\n",
    "# sparse 벡터는 dict 형태이므로 numpy vector로 변환\n",
    "def sparse_dict_to_vector(sparse_dict, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for i, token in enumerate(vocab):\n",
    "        vec[i] = sparse_dict.get(token, 0)\n",
    "    return vec\n",
    "\n",
    "vocab = list(set(sparse_encoded[\"lexical_weights\"][0].keys()) | \n",
    "             set(sparse_encoded[\"lexical_weights\"][1].keys()))\n",
    "\n",
    "v1 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][0], vocab)\n",
    "v2 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][1], vocab)\n",
    "\n",
    "sparse_sim = cosine_similarity([v1], [v2])[0][0]\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"🟦 Dense 유사도 (의미): {dense_sim:.4f}\")\n",
    "print(f\"🟧 Sparse 유사도 (단어 일치): {sparse_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟦 Dense 유사도 (의미): 0.7582\n",
      "🟧 Sparse 유사도 (단어 일치): 0.4282\n"
     ]
    }
   ],
   "source": [
    "texts = [\"내가 제일 존경하는 인물은 이순신 정군입니다.\", \"이순신 장군은 정말 위대합니다.\"]\n",
    "\n",
    "# dense-only 인코딩\n",
    "dense_encoded = bge_flagmodel.encode(texts, return_dense=True, return_sparse=False)\n",
    "\n",
    "# sparse-only 인코딩\n",
    "sparse_encoded = bge_flagmodel.encode(texts, return_dense=False, return_sparse=True)\n",
    "\n",
    "# 1. DENSE 유사도 계산 (cosine similarity)\n",
    "dense_vecs = dense_encoded[\"dense_vecs\"]\n",
    "dense_sim = cosine_similarity([dense_vecs[0]], [dense_vecs[1]])[0][0]\n",
    "\n",
    "# 2. SPARSE 유사도 계산 (lexical_weights 기반 단순 dot product)\n",
    "# sparse 벡터는 dict 형태이므로 numpy vector로 변환\n",
    "def sparse_dict_to_vector(sparse_dict, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for i, token in enumerate(vocab):\n",
    "        vec[i] = sparse_dict.get(token, 0)\n",
    "    return vec\n",
    "\n",
    "vocab = list(set(sparse_encoded[\"lexical_weights\"][0].keys()) | \n",
    "             set(sparse_encoded[\"lexical_weights\"][1].keys()))\n",
    "\n",
    "v1 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][0], vocab)\n",
    "v2 = sparse_dict_to_vector(sparse_encoded[\"lexical_weights\"][1], vocab)\n",
    "\n",
    "sparse_sim = cosine_similarity([v1], [v2])[0][0]\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"🟦 Dense 유사도 (의미): {dense_sim:.4f}\")\n",
    "print(f\"🟧 Sparse 유사도 (단어 일치): {sparse_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse 임베딩은 단어가 정확히 포함되어 있는냐를 보기때문에, 작은 오타나 표현차이도 유사도를 크게 떨어뜨릴 수 있음.   \n",
    "이 점은 dens embedding과 크게 다른 특징\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
